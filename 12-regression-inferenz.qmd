# Inferenz in linearer Regression  {#regression-inferenz}

:::{.outcomes}
- Konfidenzintervalle für Modellparameter berechnen
- Zusammenfassung eines linearen Modells in R interpretieren
- Bestimmtheitsmaß $R^2$ erklären
- Konfidenzintervalle für nicht normalverteilte Residuen berechnen
:::

Im letzten Kapitel haben wir die einfache Normalregression kennengelernt. Bevor wir die geschätzten Parameter und deren Konfidenzintervalle interpretieren, müssen wir die Annahmen der Normalregression überprüfen (hauptsächlich grafisch). Diese Annahmen sind:

**L** (linear model): der Zusammenhang zwischen Antwortvariable und Prädiktor(en) ist linear.

**I** (independen observations): die Daten sind unabhängig. Schwer zu prüfen, ergibt sich aus dem Studiendesign.

**N** (residuals normally distributed): die Residuen sind normalverteilt.

**E** (equal variance of residuals): Residuen haben eine konstante Varianz.


In diesem Kapitel beschäftigen wir uns mit folgenden Fragen:

- Wie schätzt man die Parameter?
- Wie gut sind diese Schätzungen?
- Wie gut ist das Modell?
- Was macht man, wenn die Residuen nicht normalverteilt sind?

## Zurück zum Beispiel: Anreisezeit und Arbeitszeit in der Bibliothek

### Parameter und Konfidenzintervalle schätzen
```{r}
#| message: false
#| warning: false
library(tidyverse)
library(infer)
library(knitr)
library(moderndive)
```


```{r}
#| echo: false
set.seed(123)

student_id <- 1:12000
  
anreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),
             runif(n = 12000 * 0.2, min = 60, max = 120))

geschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)

wohnort <- sapply(anreise, function(x) {
  if(x < 30) 'stadt'
  else 'land'
})

verkehrsmittel <- sapply(anreise, function(x) {
  if(x <= 10) 'zu_fuss'
  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)
  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)
  else sample(c('bus', 'auto'), size = 1)
})

zeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)

grundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)
```


```{r}
#| echo: false
set.seed(345)

befragung_size <- 200

befragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)
```

Woher kommen die Schätzungen der Modellparameter und deren Konfidenzintervalle in unserem Modell? Dieses Kapitel basiert auf den theoretischen Herleitungen aus @Fahrmeir2009.

```{r}
lin_mod <- lm(zeit_bib ~ anreise, data = befragung)

get_regression_table(lin_mod) %>% kable()
```

Eine häufig verwendete Methode, die Modellparameter zu schätzen, heißt **Methode der kleinsten Quadrate** (KQ). Sie beruht auf der Idee, dass diejenigen Modellparameter, $y$-Achsenabschnitt und die Steigung, die besten sind, bei denen die Summe der quadrierten Residuen die kleinste ist. Formal schreibt man:

:::{.def}
Gegeben sei das lineare Regressionsmodell

$$y_i=\beta_0 + \beta_1 x_i + \varepsilon_i$$

Um die unbekannten Parameter $\beta_0$ und $\beta_1$ zu bestimmen, minimiere die Summe der quadrierten Abweichungen

$$\mathrm{KQ}(\beta_0, \beta_1)=\sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2=\sum_{i=n}^n \varepsilon_i^2  \rightarrow \operatorname*{min}_{\beta_0,\, \beta_1}$$

Dann heißt
$$ \boldsymbol{\hat{\beta}}=(\hat{\beta}_0, \hat{\beta}_1)$$

der Kleinste-Quadrate-Schätzer für den Parametervektor $\boldsymbol{\beta} = (\beta_0, \beta_1)$.
:::

Es geht also darum, die **Residuen zu minimieren**, d. h. die Abstände zwischen den beobachteten und den im Modell vorhergesagten Werten so klein wie möglich zu machen. 

```{r}
#| echo: false
#| out.width: '90%'
model_res <- befragung %>%
  mutate(fitted = fitted(lin_mod), residuals = residuals(lin_mod))

ggplot(model_res, aes(x = anreise, y = zeit_bib)) +
  geom_segment(aes(xend = anreise, yend = fitted, lty = 'Residuen'), alpha = 0.2)  + 
  geom_abline(intercept = coef(lin_mod)[1], slope = coef(lin_mod)[2], color = "lightblue") +
  geom_point(aes(col = 'observed')) +
  geom_point(aes(y = fitted, col = 'fitted'), shape = 1, size = 2) +
  labs(x = 'Anreisezeit (min)', y = 'Arbeitszeit in der Bibliothek (min)') +
  scale_color_manual(name = '', values = c(observed = 'black', fitted = 'blue'), breaks = c('observed', 'fitted'), label = c('Gemessene Werte', 'Angepasste Werte')) +
  scale_linetype_manual(name = '', values = ('Residuen' = 'solid')) +
  theme(legend.position = 'bottom')
```

Man quadriert die Residuen, damit sowohl die positiven als auch die negativen gleich wichtig sind und sich in der Summe nicht gegenseitig aufheben.

Die Methode der kleinsten Quadrate ergibt folgende Formeln für die Schätzung der Modellparameter einer Einfachregression:

$$
\begin{align*}
\hat{\sigma}^2 &= \frac{1}{n-2} \sum_{i=n}^n \varepsilon_i^2 \\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x}\\
\hat{\beta}_1 &= \frac{\sum^n_{i=1} (x_i - \bar{x})(y_i - \bar{y})}{\sum^n_{i=1} (x_i - \bar{x})^2}
\end{align*}
$$
Die Residuen $\hat{\varepsilon}_i$ berechnen sich als die Differenz zwischen den beobachteten und den im Modell berechneten Werten:

$$\hat{\varepsilon}_i = y_i - \hat{y}_i$$
und die angepassten Werte als Punkte auf der Regressionsgeraden:

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$$

Die geschätzten Modellparameter sind **Zufallsvariablen**. Denn wenn die Daten etwas anders ausfallen (Zufallsstichproben!), bekommen wir andere Schätzungen für den $y$-Achsenabschnitt und die Steigung. Unter der Annahme, dass die Residuen normalverteilt sind, d. h.:

$$\varepsilon_i \sim \mathcal{N}(0,\sigma^2)$$
gilt für diese Zufallsvariablen, dass sie selbst normalverteilt sind:

$$\hat{\beta}_0 \sim \mathcal{N}(\beta_0, \sigma^2_{\hat{\beta}_0}),
 \qquad \hat{\beta}_1 \sim \mathcal{N}(\beta_1, \sigma^2_{\hat{\beta}_1})$$
 
Ihre Varianzen $\sigma^2_{\hat{\beta}_0}$ und $\sigma^2_{\hat{\beta}_1}$ müssen geschätzt werden:

$$\hat{\sigma}_{\hat{\beta}_0} = \hat{\sigma} \frac{\sqrt{\textstyle \sum_{i=1}^n x_i^2}}{\sqrt{n \textstyle \sum_{i=1}^n (x_i-\bar{x})^2}}, \quad 
\hat{\sigma}_{\hat{\beta}_1} = \frac{\hat{\sigma}}{\sqrt{\textstyle \sum_{i=1}^n (x_i-\bar{x})^2}}$$

Wenn die Residuen normalverteilt sind, kann mann zeigen, dass die sogen. standardisierten Schätzer der $t$-Verteilung folgen:

$$\frac{\hat{\beta}_0 - \beta_0}{\hat{\sigma}_{\hat{\beta}_0}} \sim t_{n-2}, \quad \frac{\hat{\beta}_1 - \beta_1}{\hat{\sigma}_{\hat{\beta}_1}} \sim t_{n-2}$$
$n-2$ steht hier für die Anzahl der Freiheitsgrade in der $t$-Verteilung. $n$ ist die Anzahl der Datenpunkte im Modell.

Somit wissen wir nun endlich, wie die Schätzungen für den $y$-Achsenabschnitt und die Steigung verteilt sind. Mit diesem Wissen können wir Konfidenzintervalle für diese Modellparameter berechnen:

$$\hat{\beta}_0 \pm \hat{\sigma}_{\hat{\beta}_0} t_{1-\alpha/2, n-2}, \qquad \hat{\beta}_1 \pm \hat{\sigma}_{\hat{\beta}_1} t_{1-\alpha/2, n-2}$$

Für $\alpha$ setzt man passendes Konfidenzlevel ein, z. B. 5%, um das 95%-Konfidenzintervall zu erhalten. 

### Wie gut ist das Modell?

Welcher Anteil der Streuung der Daten lässt sich durch die Regression erklären?

Die Streuung im Modell besteht aus:

$$
\begin{align*}
\mathit{SQT} &= \mathit{SQE} + \mathit{SQR}\\
\sum^{n}_{i = 1} (y_i-\bar{y})^2 &= \sum^{n}_{i=1} (\hat{y}_i - \bar{y})^2 + \sum^{n}_{i=1} (y_i - \hat{y}_i)^2\\
\end{align*}
$$

mit $y_i$: Messwerte, $\bar{y}$: Mittelwert, $\hat{y}_i$: angepasste Werte

- $\mathit{SQT}$ Sum of squares total: Gesamtstreuung der Daten
- $\mathit{SQE}$ Sum of squares explained: Streuung erklärt vom Modell
- $\mathit{SQR}$ Sum of sqaures residual: Residualstreuung (vom Modell nicht erklärt)

Die $\mathit{SQE}$ beschreibt die Variation der angepassten Werte um den Mittelwert der beobachteten Daten und $\mathit{SQR}$ entspricht dem nicht erklärten Teil der Streuung. Das heißt, je kleiner die Residualstreuung, desto besser das Modell, weil es dann einen größeren Anteil der Streuung der Daten erklären kann. Das Verhältnis der Gesamtstreuung zur Residualstreuung nennt man das **Bestimmtheitsmaß** (oder auch Determinationskoeffizient). Dieser wird meistens mit $R^2$ bezeichnet und ist definiert als:

$$R^2 = \frac{\mathit{SQE}}{\mathit{SQT}} = 1- \frac{\sum^{n}_{i=1} (y_i - \hat{y}_i)^2}{\sum^{n}_{i = 1} (y_i - \bar{y}_i)^2}$$

$R^2$ liegt (normalerweise) zwischen 0 (schlechtes Modell) und 1 (perfekter Fit). $R^2 < 0$ zeigt eine falsche Modellwahl (z. B. kein Achsenabschnitt, wenn dieser aber nötig ist).

Das $R^2$ hat die unangenehme Eigenschaft, bei einer multiplen Regression immer zu steigen, wenn man zusätzliche Prädiktoren hinzunimmt. Diese Prädiktoren können auch ohne jeden Zusammenhang zur abhängigen Variablen stehen. Um dieses Problem zu korrigieren, hat man das adjustierte Bestimmtheitsmaß $R^2_\text{ajd}$ eingeführt. Es "bestraft" für zusätzliche Prädiktoren
$$R^2_\text{ajd} = 1 - (1 - R^2) \frac{n-1}{n - p - 1}$$

mit $n$: Anzahl der Datenpunkte, $p$: Anzahl der Prädiktoren (ohne $y$-Achsenabschnitt). $R^2_\text{ajd}$ ist aussagekräftiger als $R^2$ bei multipler Regression.

Zurück zu unserem Beispiel. Wie viel Streuung erklärt nun unser Modell? Wir sehen uns die *tidy* Zusammenfassung an.

```{r}
get_regression_summaries(lin_mod) %>% kable()
```

- `r_squared`: Bestimmtheitsmaß $R^2$
- `adj_r_squared`: adjustiertes Bestimmtheitsmaß $R^2_\text{ajd}$
- `mse`: mittlerer quadratischer Fehler, berechnet als `mean(residuals(lin_mod)^2)`
- `rmse`: Wurzel aus `mse`
- `simga`: Standardabweichung (i.e. Standardfehler) des Fehlerterms $\varepsilon$
- `statistic`: Wert der $F$-Statistik für den Hypothesentest mit H$_0$: alle Modellparameter sind Null
- `p-value`: $p$-Wert zum Hypothesentest
- `df`: Freiheitsgrade, hier Anzahl der Prädiktoren
- `nobst`: Anzahl der Datenpunkte

Somit erklärt unser Modell 53% der Varianz der Daten.

## Bootstrap mit `infer`: Konfidenzintervall für Steigung
Die $t$-Verteilung der Schätzungen gilt *asymptotisch*, d. h. bei großen Datensätzen, auch für nicht-normalverteilte Residuen (unter bestimmten Bedingungen, s. @Fahrmeir2009). Allerdings erfordern 
**nicht-normalverteilte Residuen, große Stichproben oder aber alternative Methoden** zum Berechnen der Konfidenzintervalle. Auch heteroskedastische Residuen führen zu falschen Konfidenzintervallen und Hypothesentests. Bootstrap ist robust gegen die Verletzung beider Annahmen (Normalverteilung und Homoskedastizität der Residuen). Allerdings müssen die Daten auch hierfür unabhängig sein (keine wiederholten Messungen, keine Zeitreihen!), damit Bootstrap korrekt arbeitet.

Bei einer Einfachregression, d. h., wenn es nur einen Prädiktor gibt, ist die Steigung häufig der interessante Parameter. Wenn wir uns also nicht für den $y$-Achsenabschnitt interessieren, dann können wir das altbekannte Framework von `infer` für das Bootstrap-Konfidenzintervall für die Steigung verwenden. Für unser Beispiel ginge es so:

**Schritt 1: Bootstrap-Stichproben generieren und Statistik „Steigung“ berechnen**
```{r}
bootstrap_distn_slope <- befragung %>% 
  specify(formula = zeit_bib ~ anreise) %>%
  generate(reps = 10000, type = "bootstrap") %>% 
  calculate(stat = "slope")
```

**Schritt 2: Konfidenzintervall berechnen**
```{r}
percentile_ci <- bootstrap_distn_slope %>% 
  get_confidence_interval(type = "percentile", level = 0.95)

percentile_ci
```

**Schritt 3: Ergebnisse darstellen**
```{r, out.width = '90%'}
visualize(bootstrap_distn_slope) +
    shade_confidence_interval(endpoints = percentile_ci) 
```

Verglichen mit dem Konfidenzintervall basierend auf der Normalverteilungsannahme, ist Bootstrap hier sehr ähnlich. Das liegt daran, dass die Normalverteilungsannahme und die Homoskedastizitätsannahme ja erfüllt sind. In so einem Fall sind die Standardkonfidenzintervalle basierend auf der Normalverteilungsannahme und dem Bootstrap sehr ähnlich. Wir würden also im Normalfall einfach die Standardkonfidenzintervalle benutzten.
```{r}
get_regression_table(lin_mod) %>% 
filter(term == 'anreise') %>%
  kable()
```


Wir können mit `infer` auch einen Hypothesentest durchführen, der untersucht, ob die Steigung von null verschieden ist.

- H$_0$: $\beta_1 = 0$
- H$_1$: $\beta_1 \neq 0$

```{r}
null_distn_slope <- befragung %>% 
  specify(formula = zeit_bib ~ anreise) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 10000) %>% 
  calculate(stat = "slope")
```

Für die Darstellung benötigen wir noch die berechnete Steigung:
```{r}
observed_slope <- befragung %>% 
  specify(formula = zeit_bib ~ anreise) %>% 
  calculate(stat = "slope")

observed_slope
```

Und nun die Visualisierung:
```{r, out.width = '90%'}
visualize(null_distn_slope) +
  shade_p_value(obs_stat = observed_slope, direction = "both")

null_distn_slope %>% 
  get_p_value(obs_stat = observed_slope, direction = "both")
```


## Bootstrap mit `rsample`: Konfidenzintervalle für Steigung und $y$-Achsenabschnitt
Falls wir doch Konfidenzintervalle für beide Modellparameter brauchen (und die Annahmen Normalverteilung und/oder Homoskedastizität verletzt sind) oder aber ein lineares Modell mit mehreren Prädiktoren anpassen, können wir nicht mehr mit `infer` arbeiten. Es gibt aber ein ganzes Modelluniversum, zusammengestellt in der Paketsammlung `tidymodels` (https://www.tidymodels.org/). Darin gibt es nicht nur alle möglichen Modelle, sondern vor allem eine *tidy* und einheitliche Herangehensweise ans Modellieren. `tidymodels` ist jenseits dessen, was wir in diesem Kurs machen werden. Wir werden aber die Bibliothek `rsample` nutzen, die uns beim Bootstrappen hilft.

```{r}
library(rsample)
```

Um Konfidenzintervalle sowohl für den $y$-Achsenabschnitt als auch für die Steigung zu bekommen, generieren wir Bootstrap-Stichproben aus der `befragung`, passen dann an jede solche Stichprobe unser lineares Modell an. Jedes dieser Modelle liefert uns einen $y$-Achsenabschnitt und eine Steigung. Dadurch bekommen wir Bootstrapverteilungen dieser Modellparameter, so wie sonst auch, wenn wir Bootstrap benutzt haben. Aus diesen Bootstrapverteilungen berechnen wir dann mit der Methode der Quantile unsere Konfidenzintervalle.

**Schritt 1: Bootstrapstichproben aus `befragung` generieren**

Wir generieren 10000 Bootstrap-Stichproben aus dem Datensatz `befragung` mithilfe der Funktion `bootstraps()` aus der Bibliothek `rsample`.
```{r}
set.seed(123)

bootstrap_reps <- 10000

my_bootstraps <- bootstraps(befragung, times = bootstrap_reps)
```

Das Objekt `my_bootstraps` ist ein "verpacktes" (*nested*) Objekt. Jeder *split* ist eine Bootstrap-Stichprobe.

```{r}
my_bootstraps

class(my_bootstraps)
```

Wir sehen uns eine solche Bootstrap-Stichprobe mal an. Dazu nutzen wir die Funktion `analysis()`, die solche *splits* richtig behandelt. Jede Bootstrap-Stichprobe ist durch Ziehen mit Zurücklegen aus dem Datensatz `befragung` hervorgegangen, das ist ja beim Bootstrap immer so.
```{r}
analysis(my_bootstraps$splits[[1]])
```

Wie viele Studierende wurden mehrfach gezogen? Wir sehen uns als Beispiel die erste Bootstrap-Stichprobe an.
```{r}
analysis(my_bootstraps$splits[[1]]) %>%
  group_by(student_id) %>%
  summarise(n = n()) %>% 
  arrange(desc(n))
```

**Schritt 2: Modell auf den Bootstrap-Stichproben anpassen**

Um unser ursprüngliches lineares Modell anzupassen, benutzen wir eine selbst geschriebene Funktion, inspiriert von der Hilfe zu `rsample` (?int_pctl). Die Funktion `lm_est()` passt auf einer Bootstrap-Stichprobe das lineare Modell `zeit_bib ~ anreise` an. Um die Berechnung auf allen Bootstrap-Stichproben effizient zu machen, nutze ich die Funktion `map()`, die sich eine Bootstrap-Stichprobe nach der anderen vornimmt und das lineare Modell anpasst. Wir kommen in einer späteren Stunde auf solche effizienten Funktionen zurück.
```{r}
lm_est <- function(split, ...) {
  lm(zeit_bib ~ anreise, data = analysis(split)) %>%
    tidy()
}

model_res <- my_bootstraps %>%
  mutate(results = map(splits, .f = lm_est))
```

Wir sehen uns das entstandene Objekt mit allen angepassten Modellen an. Und $\dots$ wir sehen nichts `r emo::ji('smile')`.
```{r}
model_res
```

Die Ergebnisse müssen wir noch "auspacken."
```{r}
model_coeffs <- model_res %>%
  # Die Spalte splits los werden.
  select(-splits) %>%
  # Und das Ergebnis in ein tibble umwandeln.
  unnest(results)

model_coeffs
```

Jetzt können wir sehen, dass das Objekt `model_coeffs` die beiden Modellparameter, $y$-Achsenabschnitt und die Steigung, enthält und das jeweils so häufig, wie wir eben Bootstrap-Stichproben generiert haben. Somit haben wir jetzt eine Bootstrapverteilung dieser Modellparameter, die wir nun plotten können.
```{r, out.width = '90%'}
ggplot(model_coeffs, aes(x = estimate, group = term)) + 
  geom_histogram(col = "white", bins = 30) +
  facet_wrap(~ term, scales = "free_x")
```

**Schritt 3: Konfidenzintervalle berechnen**

Die Verteilungen sind symmetrisch, weil ja die Annahmen der Normalregression erfüllt sind. Jetzt fehlen uns noch die Konfidenzintervalle, von denen wir erwarten, dass sie den Standardkonfidenzintervallen ähnlich sein werden.

```{r}
# Konfidenzintervalle mit Bootstrap
percentile_ci <- int_pctl(model_res, results)

# Standard-Konfidenzintervalle mit der Annahme der Normalverteilung und Homoskedastizität der Residuen
standard_ci <- tidy(lin_mod, conf.int = TRUE)

percentile_ci %>% kable()
standard_ci %>% kable()
```

Wir plotten alle Ergebnisse jetzt zusammen. Das müssen wir "händisch" machen, da uns ja nicht die tolle Funktion `visualize()` aus `infer` zur Verfügung steht. Wir fügen die Konfidenzintervalle als vertikale Linien zu den Histogrammen der Bootstrapverteilungen hinzu.
```{r, out.width = '90%'}
ggplot(model_coeffs, aes(x = estimate, group = term)) + 
  geom_histogram(col = "white", bins = 30) +
  geom_vline(data = percentile_ci, aes(xintercept = .lower, group = term, col = 'percentile_ci_lower')) +
  geom_vline(data = percentile_ci, aes(xintercept = .upper, group = term), col = 'blue') +
  geom_vline(data = standard_ci, aes(xintercept = conf.low, group = term, col = 'conf.low')) +
  geom_vline(data = standard_ci, aes(xintercept = conf.high, group = term), col = 'orange') +
  scale_color_manual(name = "Confidence intervals", values = c(percentile_ci_lower = "blue", conf.low = 'orange'), labels = c('standard', 'bootstrap')) +
  facet_wrap(~ term, scales = "free_x") +
  theme(legend.position = 'bottom')
```

In diesem Beispiel ähneln sich die Konfidenzintervalle, da die Annahmen der Normalregression erfüllt sind. Das wird in den Aufgaben (s. u.), die Sie selbstständig bearbeiten werden, nicht mehr der Fall sein. Selbstverständlich sollen Sie im Falle von erfüllten Annahmen einfach die Standardkonfidenzintervalle nutzen und brauchen kein Bootstrap. Sie werden aber sehen, dass beim Modellieren mit richtigen Daten die Annahmen der Normalverteilung und der Homoskedastizität häufig verletzt sind. Mit dem Bootstrap sind Sie jetzt dafür gerüstet `r emo::ji('smile')`.


## Lesestoff
Kapitel 10 in @ModernDive


## Aufgaben

### Artenreichtum auf den Galapagosinseln
Wir beschäftigen uns mit dem Datensatz `gala` aus der Bibliothek `faraway`.

1. Laden Sie den Datensatz und lesen Sie in der Hilfe nach, worum es sich dabei handelt.
2. Untersuchen Sie die Hypothese, dass die Anzahl der endemischen Arten linear von der Gesamtartenzahl abhängt.
3. Überprüfen Sie die Annahmen des linearen Modells.
4. Vergleichen Sie das Konfidenzintervall, das auf der Normalverteilungsannahme beruht, mit dem Bootstrap-Konfidenzintervall.
1. Führen Sie den Hypothesentest analog zum Beispiel oben mit `infer` durch.
1. Interpretieren Sie das Modell.

### Artenreichtum auf den Galapagosinseln, revisited
Wiederholen Sie die obere Aufgabe nun mithilfe der Bibliothek `rasample`. Tipp: Wandeln Sie die Inselnamen, die nur als Zeilennamen existieren, in eine richtige Spalte um `gala <- gala %>% rownames_to_column(var = "island")`.

## Ihre Arbeit einreichen

- Speichern Sie Ihr Notebook ab.
- Laden Sie Ihre .Rmd Datei in ILIAS hoch. **Beachten Sie die Frist!**
- Sie erhalten die Musterlösung nach dem Hochladen.
